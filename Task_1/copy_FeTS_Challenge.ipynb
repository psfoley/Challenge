{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeTS Challenge\n",
    "\n",
    "Contributing Authors (alphabetical order):\n",
    "- Brandon Edwards (Intel)\n",
    "- Patrick Foley (Intel)\n",
    "- Alexey Gruzdev (Intel)\n",
    "- Sarthak Pati (University of Pennsylvania)\n",
    "- Micah Sheller (Intel)\n",
    "- Ilya Trushkin (Intel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "\n",
    "from fets_challenge import run_challenge_experiment\n",
    "from fets_challenge.experiment import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding custom functionality to the experiment\n",
    "Within this notebook there are **four** functional areas that you can adjust to improve upon the challenge reference code:\n",
    "\n",
    "- [Validation functions](#Custom-Validation-Functions)\n",
    "- [Custom aggregation logic](#Custom-Aggregation-Functions)\n",
    "- [Selection of training hyperparameters by round](#Custom-hyperparameters-for-training)\n",
    "- [Collaborator training selection by round](#Custom-Collaborator-Training-Selection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the standard PyTorch [validation metrics](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#classification-metrics) can be used to evaluate the model. Any user defined validation functions should conform to the following interface:\n",
    "\n",
    "    def validation_fun_interface(targets, predictions):\n",
    "        \"\"\"validation function interface\n",
    "    \n",
    "        Args:\n",
    "            Targets: numpy array of target values\n",
    "            Predictions: numpy array of predicted values by the model\n",
    "        Returns:\n",
    "            val_score : float\n",
    "        \n",
    "        return val_score\n",
    "        \n",
    "        \n",
    "Sensitivity and Specificity are defined below as reference implementations. To add custom metrics to the validation function that don't conform to the ```(targets, predictions)``` interface, [functool's partial function](https://docs.python.org/3/library/functools.html#functools.partial) can be used to fix a certain number of arguments of a function and generate a new function. \n",
    "\n",
    "The implementation of sensitivity and specificity below perform an average over these measures for each of the enhancing tumor(ET), tumor core(TC), and whole tumor(WT) regions. We utilize a function that takes the float multi-channel model output and multi-channel mask and returns binary outputs and masks for each of ET, TC,and WT.  \n",
    "\n",
    "The string that is included in the 0-index of the tuple (e.g. 'sens' or 'spec') will be assigned that name when it is stored in the aggregator's database. More information about how to use this information for custom aggregation can be found [here](#Using-validation-metrics-for-filtering)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fets_challenge.spec_sens_code import brats_labels\n",
    "\n",
    "\n",
    "def channel_sensitivity(output, target):\n",
    "    # computes TP/P for a single channel \n",
    "\n",
    "    true_positives = np.sum(output * target)\n",
    "    total_positives = np.sum(target)\n",
    "\n",
    "    if total_positives == 0:\n",
    "        score = 1.0\n",
    "    else:\n",
    "        score = true_positives / total_positives\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def channel_specificity(output, target):\n",
    "    # computes TN/N for a single channel\n",
    "\n",
    "    true_negatives = np.sum((1 - output) * (1 - target))\n",
    "    total_negatives = np.sum(1 - target)\n",
    "\n",
    "    if total_negatives == 0:\n",
    "        score = 1.0\n",
    "    else:\n",
    "        score = true_negatives / total_negatives\n",
    "        \n",
    "    return score\n",
    "   \n",
    "    \n",
    "def sensitivity(output, target):\n",
    "    \"\"\"\"\n",
    "    Calculates the average sensitivity across all of ET, TC, and WT.\n",
    "    Args:\n",
    "        Targets: numpy array of target values\n",
    "        Predictions: numpy array of predicted values by the model\n",
    "    \"\"\"        \n",
    " \n",
    "    # parsing model output and target into each of ET, TC, and WT arrays\n",
    "    brats_val_data = brats_labels(output=output, target=target)\n",
    "    \n",
    "    outputs = brats_val_data['outputs']\n",
    "    targets = brats_val_data['targets']\n",
    "    \n",
    "    output_enhancing = outputs['ET'] \n",
    "    target_enhancing = targets['ET']\n",
    "\n",
    "    output_core = outputs['TC'] \n",
    "    target_core = targets['TC'] \n",
    "\n",
    "    output_whole = outputs['WT'] \n",
    "    target_whole = targets['WT']\n",
    "\n",
    "    sensitivity_for_enhancing = channel_sensitivity(output=output_enhancing, \n",
    "                                                    target=target_enhancing)\n",
    "\n",
    "    sensitivity_for_core = channel_sensitivity(output=output_core, \n",
    "                                               target=target_core)\n",
    "\n",
    "    sensitivity_for_whole = channel_sensitivity(output=output_whole, \n",
    "                                                target=target_whole)\n",
    "\n",
    "    return (sensitivity_for_enhancing + sensitivity_for_core + sensitivity_for_whole) / 3.0\n",
    "    \n",
    "    \n",
    "def specificity(output, target):\n",
    "    \"\"\"\"\n",
    "    Calculates the average sensitivity across all of ET, TC, and WT.\n",
    "    Args:\n",
    "        Targets: numpy array of target values\n",
    "        Predictions: numpy array of predicted values by the model\n",
    "    \"\"\"  \n",
    "        \n",
    "    # parsing model output and target into each of ET, TC, and WT arrays\n",
    "    brats_val_data = brats_labels(output=output, target=target)\n",
    "    \n",
    "    outputs = brats_val_data['outputs']\n",
    "    targets = brats_val_data['targets']\n",
    "\n",
    "    \n",
    "    output_enhancing = outputs['ET'] \n",
    "    target_enhancing = targets['ET']\n",
    "\n",
    "    output_core = outputs['TC'] \n",
    "    target_core = targets['TC'] \n",
    "\n",
    "    output_whole = outputs['WT'] \n",
    "    target_whole = targets['WT']\n",
    "\n",
    "    specificity_for_enhancing = channel_specificity(output=output_enhancing, \n",
    "                                                    target=target_enhancing)\n",
    "\n",
    "    specificity_for_core = channel_specificity(output=output_core, \n",
    "                                               target=target_core)\n",
    "\n",
    "    specificity_for_whole = channel_specificity(output=output_whole, \n",
    "                                                target=target_whole)\n",
    "\n",
    "    return (specificity_for_enhancing + specificity_for_core + specificity_for_whole) / 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting access to historical weights, metrics, and more\n",
    "The **db_iterator** parameter gives full access to all of the tensors and metrics stored by the aggregator. Participants can access these records to create advanced aggregation methods, hyperparameters for training, and novel selection logic for which collaborators should participant in a given training round. See below for details about how data is stored internally and a comprehensive set of examples. \n",
    "\n",
    "## Basic Form\n",
    "Each record yielded by the db_iterator contains the following fields:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th colspan=3>TensorKey</th>\n",
    "            <th colspan=3>|</th>\n",
    "            <th>Tensor</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>'tensor_name'</td>\n",
    "            <td>'origin'</td>\n",
    "            <td>'round'</td>\n",
    "            <td>'report'</td>\n",
    "            <td>'tags'</td>\n",
    "            <td>|</td>\n",
    "            <td>'nparray'</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "All records are stored as a numpy array internally: model weights, metrics, as well as hyperparameters. \n",
    "\n",
    "Detailed field explanation:\n",
    "- **'tensor_name'** (str): The 'tensor_name' field corresponds to the model layer name (i.e. 'conv2d'), or the name of the metric that has been reported by a collaborator (i.e. 'accuracy'). The built-in validation functions used for evaluation of the challenge will be given a prefix of 'challenge_metric_\\*'. The names that you provide in conjunction with a custom validation metrics to the ```run_challenge_experiment``` function will remain unchanged.  \n",
    "- **'origin'** (str): The origin denotes where the numpy array came from. Possible values are any of the collaborator names (i.e. 'col1'), or the aggregator.\n",
    "- **'round'** (int): The round that produced the tensor. If your experiment has N rounds, possible values are 0->N-1\n",
    "- **'report'** (boolean): This field is one of the ways that a metric can be denoted; For the purpose of aggregation, this field can be ignored.\n",
    "- **'tags'** (tuple(str)): The tags include unstructured information that can be used to create complex data flows. For example, model layer weights will have the same 'tensor_name' and 'round' before and after training, so a tag of 'trained' is used to denote that the numpy array corresponds to the layer of a locally trained model. This field is also used to capture metric information. For example, aggregated_model_validation assigns tags of 'metric' and 'validate_agg' to reflect that the metric reported corresponds to the validation score of the latest aggregated model, whereas the tags of 'metric' 'validate_local' are used for metrics produced through validation after training on a collaborator's local data.   \n",
    "- **'nparray'** (numpy array) : This contains the value of the tensor. May contain the model weights, metrics, or hyperparameters as a numpy array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Collaborator Training Selection\n",
    "By default, all collaborators will be selected for training each round, but you can easily add your own logic to select a different set of collaborators based on custom criteria. An example is provided below for selecting a single collaborator on odd rounds that had the fastest training time (one_collaborator_on_odd_rounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_collaborators_train(collaborators,\n",
    "                            db_iterator,\n",
    "                            fl_round,\n",
    "                            collaborators_chosen_each_round,\n",
    "                            collaborator_times_per_round):\n",
    "    \"\"\"Chooses which collaborators will train for a given round.\n",
    "    \n",
    "    Args:\n",
    "        collaborators: list of strings of collaborator names\n",
    "        db_iterator: iterator over history of all tensors.\n",
    "            Columns: ['tensor_name', 'round', 'tags', 'nparray']\n",
    "        fl_round: round number\n",
    "        collaborators_chosen_each_round: a dictionary of {round: list of collaborators}. Each list indicates which collaborators trained in that given round.\n",
    "        collaborator_times_per_round: a dictionary of {round: {collaborator: total_time_taken_in_round}}.  \n",
    "    \"\"\"\n",
    "    return collaborators\n",
    "\n",
    "# this is not a good algorithm, but we include it to demonstrate the following:\n",
    "# MICAH TODO FILL IN\n",
    "def one_collaborator_on_odd_rounds(collaborators,\n",
    "                                   db_iterator,\n",
    "                                   fl_round,\n",
    "                                   collaborators_chosen_each_round,\n",
    "                                   collaborator_times_per_round):\n",
    "    \"\"\"Chooses which collaborators will train for a given round.\n",
    "    \n",
    "    Args:\n",
    "        collaborators: list of strings of collaborator names\n",
    "        db_iterator: iterator over history of all tensors.\n",
    "            Columns: ['tensor_name', 'round', 'tags', 'nparray']\n",
    "        fl_round: round number\n",
    "        collaborators_chosen_each_round: a dictionary of {round: list of collaborators}. Each list indicates which collaborators trained in that given round.\n",
    "        collaborator_times_per_round: a dictionary of {round: {collaborator: total_time_taken_in_round}}.  \n",
    "    \"\"\"\n",
    "    # on odd rounds, choose the fastest from the previous round\n",
    "    if fl_round % 2 == 1:\n",
    "        training_collaborators = None\n",
    "        fastest_time = np.inf\n",
    "        for col, t in collaborator_times_per_round[fl_round - 1].items():\n",
    "            if t < fastest_time:\n",
    "                fastest_time = t\n",
    "                training_collaborators = [col]\n",
    "    else:\n",
    "        training_collaborators = collaborators\n",
    "    return training_collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom hyperparameters for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training hyperparameters for a round should return a dictionary with **'epochs_per_round'** and **'learning_rate'** as keys. Anything specified otherwise will raise an exception. These hyperparameters are set for all the collaborators chosen for that round. For example, if you set epoch_per_round to 2.0, all collaborator selected based on the [collaborator training selection criteria](#Custom-Collaborator-Training-Selection) will train for two epochs. Different hyperparameters can be specified for collaborators for different rounds but they remain the same for all the collaborators that are chosen for that particular round. In simpler words, collaborators can not have different hyperparameters for the same round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wacky_hyper_parameters_for_round(collaborators,\n",
    "                                        db_iterator, # this will actually contain the hyper-parameter history as well\n",
    "                                        fl_round,\n",
    "                                        collaborators_chosen_each_round,\n",
    "                                        collaborator_times_per_round):\n",
    "    \"\"\"Set the training hyper-parameters for the round.\n",
    "    \n",
    "    Args:\n",
    "        collaborators: list of strings of collaborator names\n",
    "        db_iterator: iterator over history of all tensors.\n",
    "            Columns: ['tensor_name', 'round', 'tags', 'nparray']\n",
    "        fl_round: round number\n",
    "        collaborators_chosen_each_round: a dictionary of {round: list of collaborators}. Each list indicates which collaborators trained in that given round.\n",
    "        collaborator_times_per_round: a dictionary of {round: {collaborator: total_time_taken_in_round}}.  \n",
    "    Returns:\n",
    "        a dictionary with optional keys from:\n",
    "            - epochs_per_round \n",
    "            - batches_per_round (takes priority over \"epochs_per_round\")\n",
    "            - learning_rate\n",
    "            - TODO: others???\n",
    "    \"\"\"\n",
    "    # adaptive epochs\n",
    "    if fl_round == 0:\n",
    "        epochs = 0.5\n",
    "    elif fl_round < 3:\n",
    "        epochs = 1.5\n",
    "    else:\n",
    "        epochs = 1.0\n",
    "        \n",
    "    # useless round 0\n",
    "    if fl_round == 0:\n",
    "        learning_rate = 1e-10\n",
    "    else:\n",
    "        learning_rate = 1e-4\n",
    "    \n",
    "    # MICAH TODO: make them return everything always, but make epochs/batches mutually-exclusive.\n",
    "    \n",
    "    return {'epochs_per_round': epochs, 'learning_rate': learning_rate}\n",
    "\n",
    "def training_hyper_parameters_for_round(collaborators,\n",
    "                                        db_iterator, # this will actually contain the hyper-parameter history as well\n",
    "                                        fl_round,\n",
    "                                        collaborators_chosen_each_round,\n",
    "                                        collaborator_times_per_round):\n",
    "    \"\"\"Set the training hyper-parameters for the round.\n",
    "    \n",
    "    Args:\n",
    "        collaborators: list of strings of collaborator names\n",
    "        db_iterator: iterator over history of all tensors.\n",
    "            Columns: ['tensor_name', 'round', 'tags', 'nparray']\n",
    "        fl_round: round number\n",
    "        collaborators_chosen_each_round: a dictionary of {round: list of collaborators}. Each list indicates which collaborators trained in that given round.\n",
    "        collaborator_times_per_round: a dictionary of {round: {collaborator: total_time_taken_in_round}}.  \n",
    "    Returns:\n",
    "        a dictionary with optional keys from:\n",
    "            - epochs_per_round \n",
    "            - batches_per_round (takes priority over \"epochs_per_round\")\n",
    "            - learning_rate\n",
    "    \"\"\"\n",
    "    # this will use the defaults of\n",
    "    # MICAH TODO\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Aggregation Functions\n",
    "Standard aggregation methods allow for simple layer-wise combination (via weighted_mean, mean, median, etc.); however, more complex aggregation methods can be supported by evaluating collaborator metrics, weights from prior rounds, etc. User provided custom aggregation functions should implement the [**AggregationFunctionInterface**](https://github.com/intel/openfl/blob/fets/openfl/component/aggregation_functions/interface.py). \n",
    "\n",
    "[**LocalTensors**](https://github.com/intel/openfl/blob/fets/openfl/utilities/types.py#L13) are named tuples of the form ('collaborator_name', 'tensor', 'collaborator_weight'). Your custom aggregation function will be passed a list of LocalTensors, which will contain an entry for each collaborator who participated in the prior training round. The [**db_iterator**](#Getting-access-to-historical-weights,-metrics,-and-more) can be used to construct complex aggregation methods. A few examples are included below.\n",
    "\n",
    "## db_iterator aggregation examples\n",
    "### Using prior layer weights\n",
    "Here is an example of how to extract layer weights from prior round. The tag is 'aggregated' indicates this : \n",
    "    \n",
    "    for record in db_iterator:\n",
    "            if (\n",
    "                record['round'] == (fl_round - 1)\n",
    "                and record['tensor_name'] == tensor_name\n",
    "                and 'aggregated' in record['tags']\n",
    "                and 'delta' not in record['tags']\n",
    "               ):\n",
    "                previous_tensor_value = record['nparray']\n",
    "                break\n",
    "\n",
    "### Using validation metrics for filtering\n",
    "\n",
    "    threshold = fl_round * 0.3 + 0.5\n",
    "    metric_name = 'acc'\n",
    "    tags = ('metric','validate_agg')\n",
    "    selected_tensors = []\n",
    "    selected_weights = []\n",
    "    for record in db_iterator:\n",
    "        for local_tensor in local_tensors:\n",
    "            tags = set(tags + [local_tensor.col_name])\n",
    "            if (\n",
    "                tags <= set(record['tags']) \n",
    "                and record['round'] == fl_round\n",
    "                and record['tensor_name'] == metric_name\n",
    "                and record['nparray'] >= threshold\n",
    "            ):\n",
    "                selected_tensors.append(local_tensor.tensor)\n",
    "                selected_weights.append(local_tensor.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.component.aggregation_functions import AggregationFunctionInterface\n",
    "import numpy as np\n",
    "\n",
    "class ClippedAveraging(AggregationFunctionInterface):\n",
    "    def __init__(self, ratio):\n",
    "        self.ratio = ratio\n",
    "        \n",
    "    def call(self,\n",
    "             local_tensors,\n",
    "             db_iterator,\n",
    "             tensor_name,\n",
    "             fl_round,\n",
    "             *__):\n",
    "        \"\"\"Aggregate tensors.\n",
    "\n",
    "        Args:\n",
    "            local_tensors(list[openfl.utilities.LocalTensor]): List of local tensors to aggregate.\n",
    "            db_iterator: iterator over history of all tensors.\n",
    "                Columns: ['tensor_name', 'round', 'tags', 'nparray']\n",
    "            tensor_name: name of the tensor\n",
    "            fl_round: round number\n",
    "            tags: tuple of tags for this tensor\n",
    "        \"\"\"\n",
    "        clipped_tensors = []\n",
    "        previous_tensor_value = None\n",
    "        for record in db_iterator:\n",
    "            if (\n",
    "                record['round'] == (fl_round - 1)\n",
    "                and record['tensor_name'] == tensor_name\n",
    "                and 'aggregated' in record['tags']\n",
    "                and 'delta' not in record['tags']\n",
    "               ):\n",
    "                previous_tensor_value = record['nparray']\n",
    "                break\n",
    "        weights = []\n",
    "        for local_tensor in local_tensors:\n",
    "            prev_tensor = previous_tensor_value if previous_tensor_value is not None else local_tensor.tensor\n",
    "            delta = local_tensor.tensor - prev_tensor\n",
    "            new_tensor = prev_tensor + clip(delta, self.ratio)\n",
    "            clipped_tensors.append(new_tensor)\n",
    "            weights.append(local_tensor.weight)\n",
    "\n",
    "        return np.average(clipped_tensors, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Experiment\n",
    "\n",
    "```run_challenge_experiment``` is singular interface where your custom methods can be passed.\n",
    "\n",
    "- ```aggregation_function```, ```choose_training_collaborators```, ```training_hyper_parameters_for_round```, and ```validation_functions``` correspond to the [this list](#Custom-hyperparameters-for-training) of configurable functions \n",
    "described within this notebook.\n",
    "- ```institution_split_csv_filename``` : Describes how the data should be split between all collaborators. Extended documentation about configuring the splits in the ```institution_split_csv_filename``` parameter can be found in the [README.md](https://github.com/FETS-AI/Challenge/blob/main/Task_1/README.md). \n",
    "- ```db_store_rounds``` : This parameter determines how long metrics and weights should be stored by the aggregator before being deleted. Providing a value of -1 will result in all historical data being retained, but memory usage will likely increase.\n",
    "- ```rounds_to_train``` : Defines how many rounds will occur in the experiment\n",
    "- ```device``` : Which device to use for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'TrainOrVal' column found in split_subdirs csv, so performing automated split using percent_train of 0.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[10:03:05] </span><span style=\"color: #000080\">INFO</span>     Updating aggregator.settings.rounds_to_train to <span style=\"color: #000080; font-weight: bold\">5</span><span style=\"color: #808000\">...</span>                                                                            <a href=\"file:///home/edwardsb/virtual/fets_challenge_test_2/lib/python3.6/site-packages/openfl/native/native.py\"><span style=\"color: #7f7f7f\">native.py</span></a><span style=\"color: #7f7f7f\">:83</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f42501924e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">           </span><span style=\"color: #000080\">INFO</span>     Updating aggregator.settings.db_store_rounds to <span style=\"color: #000080; font-weight: bold\">5</span><span style=\"color: #808000\">...</span>                                                                            <a href=\"file:///home/edwardsb/virtual/fets_challenge_test_2/lib/python3.6/site-packages/openfl/native/native.py\"><span style=\"color: #7f7f7f\">native.py</span></a><span style=\"color: #7f7f7f\">:83</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f458c0b7d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edwardsb/virtual/fets_challenge_test_2/lib/python3.6/site-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "out_conv will be using final activation:  sigmoid\n",
      "\n",
      "out_conv will be using sigmoid_input_multiplier:  20.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[10:03:25] </span><span style=\"color: #800000\">WARNING</span>  tried to remove tensor: __opt_state_needed not present in the tensor dict                                                        <a href=\"file:///home/edwardsb/virtual/fets_challenge_test_2/lib/python3.6/site-packages/openfl/utilities/utils.py\"><span style=\"color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f\">:86</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f4575495828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[10:03:26] </span><span style=\"color: #000080\">INFO</span>     Creating aggregator<span style=\"color: #808000\">...</span>                                                                                                     <a href=\"file:///home/edwardsb/repositories/Challenge/Task_1/fets_challenge/experiment.py\"><span style=\"color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f\">:279</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f450f4ff4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">[10:03:27] </span><span style=\"color: #000080\">INFO</span>     Creating collaborators<span style=\"color: #808000\">...</span>                                                                                                  <a href=\"file:///home/edwardsb/repositories/Challenge/Task_1/fets_challenge/experiment.py\"><span style=\"color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f\">:283</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f450e27c128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">           </span><span style=\"color: #000080\">INFO</span>     Starting experiment                                                                                                        <a href=\"file:///home/edwardsb/repositories/Challenge/Task_1/fets_challenge/experiment.py\"><span style=\"color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f\">:291</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f450f51cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf\">           </span><span style=\"color: #000080\">INFO</span>     Collaborators chosen to train for round <span style=\"color: #000080; font-weight: bold\">0</span>:                                                                                 <a href=\"file:///home/edwardsb/repositories/Challenge/Task_1/fets_challenge/experiment.py\"><span style=\"color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f\">:305</span>\n",
       "                            <span style=\"font-weight: bold\">[</span><span style=\"color: #008000\">'01'</span>, <span style=\"color: #008000\">'02'</span>, <span style=\"color: #008000\">'03'</span>, <span style=\"color: #008000\">'04'</span>, <span style=\"color: #008000\">'05'</span>, <span style=\"color: #008000\">'06'</span>, <span style=\"color: #008000\">'07'</span>, <span style=\"color: #008000\">'08'</span>, <span style=\"color: #008000\">'09'</span>, <span style=\"color: #008000\">'10'</span>, <span style=\"color: #008000\">'11'</span>, <span style=\"color: #008000\">'12'</span>, <span style=\"color: #008000\">'13'</span>, <span style=\"color: #008000\">'14'</span>, <span style=\"color: #008000\">'15'</span>, <span style=\"color: #008000\">'16'</span>, <span style=\"color: #008000\">'17'</span><span style=\"font-weight: bold\">]</span>                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f450f4ff780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-78bd85a8dd1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                          \u001b[0mdb_store_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb_store_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                          \u001b[0mrounds_to_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrounds_to_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                          device=device)\n\u001b[0m",
      "\u001b[0;32m~/repositories/Challenge/Task_1/fets_challenge/experiment.py\u001b[0m in \u001b[0;36mrun_challenge_experiment\u001b[0;34m(aggregation_function, choose_training_collaborators, training_hyper_parameters_for_round, validation_functions, institution_split_csv_filename, brats_training_data_parent_dir, db_store_rounds, rounds_to_train, device)\u001b[0m\n\u001b[1;32m    315\u001b[0m                                                       collaborator_times_per_round)\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_per_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs_per_round\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatches_per_round\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
     ]
    }
   ],
   "source": [
    "# TODO: add commments for each below\n",
    "aggregation_function=ClippedAveraging(ratio=1.0)\n",
    "choose_training_collaborators=all_collaborators_train\n",
    "training_hyper_parameters_for_round=training_hyper_parameters_for_round\n",
    "validation_functions=[('spec', specificity), ('sens', sensitivity)]\n",
    "institution_split_csv_filename='partitioning_1.csv'\n",
    "brats_training_data_parent_dir='/raid/datasets/FeTS21/MICCAI_FeTS2021_TrainingData'\n",
    "db_store_rounds=5\n",
    "rounds_to_train=5\n",
    "device='cuda'\n",
    "\n",
    "\n",
    "run_challenge_experiment(aggregation_function=aggregation_function,\n",
    "                         choose_training_collaborators=choose_training_collaborators,\n",
    "                         training_hyper_parameters_for_round=training_hyper_parameters_for_round,\n",
    "                         validation_functions=validation_functions,\n",
    "                         institution_split_csv_filename=institution_split_csv_filename,\n",
    "                         brats_training_data_parent_dir=brats_training_data_parent_dir,\n",
    "                         db_store_rounds=db_store_rounds,\n",
    "                         rounds_to_train=rounds_to_train,\n",
    "                         device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fets_challenge_test_2",
   "language": "python",
   "name": "fets_challenge_test_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
